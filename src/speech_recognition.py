"""
è¯­éŸ³è¯†åˆ«æ¨¡å—
ä½¿ç”¨ Whisper æ¨¡å‹è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
"""

import io
import soundfile as sf
import numpy as np
from faster_whisper import WhisperModel
from config import (
    WHISPER_MODEL_PATH, 
    WHISPER_DEVICE, 
    WHISPER_COMPUTE_TYPE,
    SAMPLE_RATE,
    SILENCE_DURATION,
    VERBOSE
)

def log(msg):
    if VERBOSE:
        print(msg)

class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«å™¨"""
    
    def __init__(self):
        log("ğŸ”„ æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹...")
        self.model = WhisperModel(
            WHISPER_MODEL_PATH, 
            device=WHISPER_DEVICE, 
            compute_type=WHISPER_COMPUTE_TYPE
        )
        log("âœ… Whisper æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def transcribe(self, audio_data: np.ndarray) -> str:
        """å°†éŸ³é¢‘æ•°æ®è½¬å½•ä¸ºæ–‡æœ¬"""
        if audio_data is None or len(audio_data) == 0:
            return ""
        
        try:
            # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸º WAV æ ¼å¼
            buf = io.BytesIO()
            sf.write(buf, audio_data, SAMPLE_RATE, format="WAV")
            buf.seek(0)
            
            # ä½¿ç”¨ Whisper è¿›è¡Œè½¬å½•
            segments, _ = self.model.transcribe(
                buf,
                vad_filter=True,
                vad_parameters={"min_silence_duration_ms": int(SILENCE_DURATION * 1000)},
                beam_size=5
            )
            
            # åˆå¹¶æ‰€æœ‰ç‰‡æ®µçš„æ–‡æœ¬
            text = " ".join(segment.text for segment in segments)
            return text.strip()
            
        except Exception as e:
            log(f"âŒ è½¬å½•é”™è¯¯: {e}")
            return ""
